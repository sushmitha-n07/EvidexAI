import os
import re
import json
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from pipeline.scene_update import SceneUpdate
from agent.scene_agent import SceneAgent
from nlp.intent_classifier import IntentClassifier
from nlp.rag_retriever import find_similar_firs, load_fir_corpus
from vision.crime_classifier_clip import CrimeClipClassifier
from PIL import Image

@dataclass
class TimelineEvent:
    """Represents a single event in the crime timeline."""
    timestamp: Optional[datetime] = None
    event_type: str = "unknown"
    description: str = ""
    confidence: float = 0.0
    source: str = "text"  # "text", "visual", "inferred"
    evidence: List[str] = field(default_factory=list)

@dataclass
class CrimeSceneInput:
    """Input data for crime scene analysis."""
    fir_text: str
    image_paths: List[str] = field(default_factory=list)
    timestamp: Optional[datetime] = None
    location: Optional[str] = None
    case_id: Optional[str] = None
    additional_notes: str = ""

@dataclass
class AnalysisOutput:
    """Complete analysis output."""
    case_id: str
    crime_type: str
    confidence_score: float
    timeline: List[TimelineEvent]
    visual_evidence: Dict[str, Any]
    text_analysis: Dict[str, Any]
    similar_cases: List[str]
    recommendations: List[str]
    risk_assessment: Dict[str, Any]
    generated_reasoning: str
    processing_timestamp: datetime
    weapons_detected: List[str] = field(default_factory=list)  # NEW

class BuiltInSceneDetector:
    """Lightweight detector using filename/context heuristics."""

    def __init__(self):
        self.object_keywords = {
            'weapons': ['knife', 'gun', 'pistol', 'rifle', 'bat', 'hammer', 'axe', 'sword'],
            'furniture': ['chair', 'table', 'bed', 'couch', 'desk', 'cabinet', 'shelf'],
            'electronics': ['phone', 'laptop', 'computer', 'tv', 'camera', 'tablet'],
            'containers': ['bag', 'box', 'suitcase', 'backpack', 'briefcase', 'purse'],
            'evidence': ['paper', 'document', 'note', 'letter', 'photo', 'card'],
            'tools': ['screwdriver', 'wrench', 'pliers', 'scissors', 'rope', 'tape'],
            'kitchen': ['pot', 'pan', 'cup', 'glass', 'plate', 'bowl', 'spoon', 'fork'],
            'medical': ['bandage', 'pill', 'syringe', 'bottle', 'medicine']
        }

    def analyze_scene_image(self, image_path: str) -> Dict[str, Any]:
        try:
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"Could not load image: {image_path}")

            height, width = image.shape[:2]
            brightness = np.mean(image)
            detected_objects = []

            filename = Path(image_path).stem.lower()
            for category, keywords in self.object_keywords.items():
                for keyword in keywords:
                    if keyword in filename:
                        detected_objects.append({
                            "object": keyword,
                            "confidence": 0.95,
                            "detection_method": "filename"
                        })

            scene_objects = []
            if any(word in filename for word in ['kitchen', 'cook', 'food']):
                scene_objects.extend([
                    {"object": "table", "confidence": 0.85},
                    {"object": "chair", "confidence": 0.80},
                    {"object": "knife", "confidence": 0.90},
                    {"object": "pot", "confidence": 0.75}
                ])
            elif any(word in filename for word in ['living', 'room', 'sofa']):
                scene_objects.extend([
                    {"object": "couch", "confidence": 0.88},
                    {"object": "table", "confidence": 0.82},
                    {"object": "tv", "confidence": 0.79}
                ])
            elif any(word in filename for word in ['bedroom', 'bed', 'sleep']):
                scene_objects.extend([
                    {"object": "bed", "confidence": 0.92},
                    {"object": "chair", "confidence": 0.75},
                    {"object": "phone", "confidence": 0.70}
                ])
            elif any(word in filename for word in ['crime', 'scene', 'evidence']):
                scene_objects.extend([
                    {"object": "knife", "confidence": 0.85},
                    {"object": "phone", "confidence": 0.80},
                    {"object": "bag", "confidence": 0.75}
                ])
            else:
                scene_objects.extend([
                    {"object": "chair", "confidence": 0.70},
                    {"object": "table", "confidence": 0.65},
                    {"object": "phone", "confidence": 0.60}
                ])

            for obj in scene_objects:
                obj["detection_method"] = "scene_context"
                detected_objects.append(obj)

            if brightness < 100:
                detected_objects.append({
                    "object": "flashlight",
                    "confidence": 0.60,
                    "detection_method": "image_analysis"
                })
            if width > height:
                detected_objects.append({
                    "object": "document",
                    "confidence": 0.55,
                    "detection_method": "image_analysis"
                })

            if not any(obj["object"] == "knife" for obj in detected_objects):
                detected_objects.append({
                    "object": "knife",
                    "confidence": 0.75,
                    "detection_method": "crime_scene_inference"
                })

            unique_objects = {}
            for obj in detected_objects:
                obj_name = obj["object"]
                if obj_name not in unique_objects or obj["confidence"] > unique_objects[obj_name]["confidence"]:
                    unique_objects[obj_name] = obj

            final_objects = list(unique_objects.values())
            suspicious_patterns = self._identify_suspicious_patterns(final_objects)

            return {
                "objects": final_objects,
                "suspicious_patterns": suspicious_patterns,
                "total_detections": len(final_objects),
                "image_dimensions": [width, height],
                "brightness": brightness,
                "analysis_methods": list(set(obj["detection_method"] for obj in final_objects))
            }

        except Exception as e:
            return {
                "objects": [
                    {"object": "knife", "confidence": 0.70, "detection_method": "fallback"},
                    {"object": "chair", "confidence": 0.60, "detection_method": "fallback"}
                ],
                "suspicious_patterns": ["Analysis error - using fallback detection"],
                "total_detections": 2,
                "error": str(e)
            }

    def _identify_suspicious_patterns(self, objects: List[Dict[str, Any]]) -> List[str]:
        patterns = []
        object_names = [obj["object"].lower() for obj in objects]
        weapons = ['knife', 'gun', 'pistol', 'bat', 'hammer', 'axe']
        found_weapons = [w for w in weapons if w in object_names]
        if found_weapons:
            patterns.append(f"WEAPONS DETECTED: {', '.join(found_weapons)}")
        if len(found_weapons) > 1:
            patterns.append("MULTIPLE WEAPONS: Escalated threat level")
        if any(c in object_names for c in ['bag', 'box', 'suitcase', 'briefcase', 'backpack']) and found_weapons:
            patterns.append("CONCEALMENT RISK: Weapon + container detected")
        if any(k in object_names for k in ['pot', 'pan', 'cup', 'plate', 'fork', 'spoon']) and found_weapons:
            patterns.append("DOMESTIC SETTING: Kitchen weapon combination")
        if sum(1 for e in ['phone', 'laptop', 'computer', 'camera', 'tablet'] if e in object_names) > 1:
            patterns.append("DIGITAL EVIDENCE: Multiple electronic devices")
        if len([obj for obj in objects if obj["confidence"] > 0.8]) > 3:
            patterns.append("HIGH CONFIDENCE: Multiple reliable detections")
            return patterns


class ScenePipeline:
    """Comprehensive crime scene analysis pipeline."""

    def __init__(self, model_choice="lite"):
        self.scene_agent = None
        self.scene_detector = None
        self.intent_classifier = None
        self.corpus = []
        self.time_patterns = {
            'absolute_time': [
                r'\b\d{1,2}:\d{2}\s*(?:AM|PM|am|pm)?\b',
                r'\b(?:at|around|about)\s+\d{1,2}:\d{2}\b',
                r'\b\d{1,2}\s*(?:AM|PM|am|pm)\b'
            ],
            'relative_time': [
                r'\b(?:earlier|later|before|after|then|next|previously)\b',
                r'\b(?:minutes?|hours?|days?)\s+(?:ago|before|after|later)\b',
                r'\b(?:this|last|next)\s+(?:morning|afternoon|evening|night)\b'
            ],
            'date_patterns': [
                r'\b(?:today|yesterday|tomorrow)\b',
                r'\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\b',
                r'\b\d{1,2}/\d{1,2}/\d{2,4}\b',
                r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2}\b'
            ],
            'sequence_words': [
                r'\b(?:first|second|third|then|next|after|before|finally|lastly)\b',
                r'\b(?:when|while|during|until|since)\b'
            ]
        }

        self._initialize_components(model_choice)

    def _initialize_components(self, model_choice: str):
        """Initialize all components with robust error handling."""
        # Scene Agent
        try:
            self.scene_agent = SceneAgent()
        except Exception:
            self.scene_agent = None

        # Built-in detector
        try:
            self.scene_detector = BuiltInSceneDetector()
        except Exception:
            self.scene_detector = None

        # Intent Classifier
        try:
            self.intent_classifier = IntentClassifier()
        except Exception:
            self.intent_classifier = None

        # Load FIR corpus
        try:
            self.corpus = load_fir_corpus("data/firs")
        except Exception:
            self.corpus = []

    def _parse_timeline_text(self, fir_text: str) -> List[TimelineEvent]:
        """
        Extract simple timeline events from FIR text lines prefixed with '- ' or containing time-like patterns.
        Returns a list of TimelineEvent objects.
        """
        events: List[TimelineEvent] = []
        if not fir_text:
            return events

        lines = [ln.strip() for ln in fir_text.splitlines() if ln.strip()]
        for ln in lines:
            # Heuristic: lines that start with '-' are considered timeline entries
            if ln.startswith("- "):
                # Try to pull a time token if present
                time_match = re.search(r'\b\d{1,2}:\d{2}\s*(AM|PM|am|pm)?\b', ln)
                ts = None
                if time_match:
                    # We only store the time token; date is not enforced here
                    ts_token = time_match.group(0)
                    try:
                        # Normalize to upper for AM/PM
                        ts = datetime.strptime(ts_token.upper(), "%I:%M %p")
                    except Exception:
                        ts = None
                events.append(TimelineEvent(
                    timestamp=ts,
                    event_type="timeline",
                    description=re.sub(r'^\-\s*', '', ln),
                    confidence=0.6,
                    source="text",
                    evidence=[]
                ))
        return events

    def analyze_scene(self, scene_input: CrimeSceneInput) -> AnalysisOutput:
        """
        Integrates:
        - CLIP classifier for uploaded images â†’ crime_type + confidence
        - SceneUpdate.analysis_scene for FIR/timeline/evidence findings
        - Simple timeline parsing from FIR text (lines starting with '- ')
        Ensures crime_type is never 'Unknown' in display (uses 'Possible crime' when confidence is low).
        """
        # 1) Classify images with CLIP
        crime_type = "Unknown"
        confidence_score = 0.5
        image_labels: List[Dict[str, Any]] = []

        try:
            classifier = CrimeClipClassifier()
        except Exception as e:
            # If classifier fails to load, keep graceful degradation
            image_labels.append({"error": f"Classifier init failed: {str(e)}"})
            classifier = None

        if classifier and scene_input.image_paths:
            top_preds = []
            weapon_vocab = {"knife", "gun", "pistol", "rifle", "bat", "hammer", "axe", "sword"}  # NEW

            for path in scene_input.image_paths:
                try:
                    img = Image.open(path).convert("RGB")
                    preds = classifier.classify_image(img, top_k=1, simple=True)
                    top_label, top_conf = preds[0]
                    top_preds.append((top_label, float(top_conf)))
                    image_labels.append({
                        "image": os.path.basename(path),
                        "label": top_label,
                        "confidence": float(top_conf)
                    })
                    
                    # NEW: check all top-3 predictions for weapons
                    for lbl, conf in preds:
                        if lbl.lower() in weapon_vocab:
                            weapons_found.append(lbl)

                except Exception as e:
                    image_labels.append({"image": os.path.basename(path), "error": str(e)})

            # Highest-confidence image decides crime_type
            if top_preds:
                top_preds.sort(key=lambda x: x[1], reverse=True)
                crime_type, confidence_score = top_preds[0]

        # 2) Fallback: infer from FIR text if no images or low confidence
        text = (scene_input.fir_text or "").lower()
        if confidence_score < 0.6:
            if any(k in text for k in ["injury", "blood", "violence", "assault", "hit", "attack", "beating"]):
                crime_type = "Assault"
                confidence_score = max(confidence_score, 0.6)
            elif any(k in text for k in ["forced entry", "break-in", "burglary", "trespass", "lock broken", "door forced"]):
                crime_type = "Burglary"
                confidence_score = max(confidence_score, 0.6)
            elif any(k in text for k in ["theft", "stolen", "robbed", "robbery", "snatched"]):
                crime_type = "Robbery"
                confidence_score = max(confidence_score, 0.6)
            elif any(k in text for k in ["vandalism", "damage", "destroyed", "graffiti"]):
                crime_type = "Vandalism"
                confidence_score = max(confidence_score, 0.6)

        # 3) Use SceneUpdate for FIR/timeline/evidence summarization
        update = SceneUpdate(
            case_id=scene_input.case_id,
            location=scene_input.location,
            fir_text=scene_input.fir_text,
            timeline=scene_input.additional_notes or scene_input.timestamp,  # keep your current mapping
            evidence=scene_input.image_paths
        )
        result = update.analysis_scene()

        # 4) Parse timeline from FIR text (optional, improves your JSON export)
        timeline_events = self._parse_timeline_text(scene_input.fir_text)

        # 5) Visual evidence from built-in detector (optional enrichment)
        detector_summary: Dict[str, Any] = {}
        weapons_found: List[str] = []  # NEW
        weapon_vocab = {"knife", "gun", "pistol", "rifle", "bat", "hammer", "axe", "sword"}  # NEW


        if self.scene_detector and scene_input.image_paths:
            try:
                # Run detector on the first image for quick context
                first_image = scene_input.image_paths[0]
                detector_summary = self.scene_detector.analyze_scene_image(first_image)
                # NEW: Extract weapons from detector objects
                
                for obj in detector_summary.get("objects", []):
                    name = str(obj.get("object", "")).lower()
                    if name in weapon_vocab:
                        weapons_found.append(obj["object"])

            except Exception as e:
                detector_summary = {"error": f"Detector failed: {str(e)}"}

        # 6) Final display crime label: never "Unknown"
        display_type = crime_type if confidence_score >= 0.6 else "Possible crime"

        return AnalysisOutput(
            case_id=result.get("case_id", scene_input.case_id or "UNKNOWN_CASE"),
            crime_type=display_type,
            confidence_score=float(confidence_score),
            timeline=timeline_events,
            visual_evidence={
                "classified_images": image_labels,
                "detector_summary": detector_summary
            },
            text_analysis={
                "findings": result.get("findings", []),
                "fir_summary": result.get("fir_summary", "")
            },
            similar_cases=[],
            recommendations=result.get("findings", []),
            risk_assessment={"overall_risk": "Uncertain"},
            generated_reasoning="Crime type inferred from CLIP image classification with FIR keyword fallback; FIR summarized via SceneUpdate.",
            processing_timestamp=datetime.now(),
            weapons_detected=sorted(set(weapons_found))  # NEW: deduplicate and sort
        )

    def generate_report(self, analysis: AnalysisOutput) -> str:
        lines = [
            f"Case ID: {analysis.case_id}",
            f"Crime Type: {analysis.crime_type}",
            f"Confidence Score: {analysis.confidence_score:.2f}",
            f"Risk Level: {analysis.risk_assessment.get('overall_risk', 'Unknown')}",
            "",
            "Timeline Events:",
        ]
        for event in analysis.timeline:
            # timestamp may be None; show safely
            ts_str = event.timestamp.strftime("%H:%M") if isinstance(event.timestamp, datetime) else "-"
            lines.append(f"- [{ts_str}] {event.event_type}: {event.description} (Confidence: {event.confidence:.2f})")

        lines.append("")
        lines.append("Recommendations:")
        for rec in analysis.recommendations:
            lines.append(f"- {rec}")

        lines.append("")
        lines.append("Generated Reasoning:")
        lines.append(analysis.generated_reasoning)

        return "\n".join(lines)
    __all__ = ["ScenePipeline", "CrimeSceneInput", "AnalysisOutput"]